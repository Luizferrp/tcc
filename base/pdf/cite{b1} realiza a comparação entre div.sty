\cite{b1} realiza a comparação entre diversas abordagens de aprendizado de máquina, diferentes misturas de features para os algoritmos de classificação, abordando as  vantagens e desvantagens do uso de cada uma das técnicas e ferramentas, realizando um estudo
    comparativo da abordagem de classificadores

    \cite{b2} Realiza um estudo de sistemas de detecção de intrusão em redes utilizando artigos e conhecimentos recentes como o progresso do uso de Aprendizado de máquina e inteligência artificial no campo dos sistemas de detecção de intrusão, bem como uma análise criteriosa de performance, bases utilizadas e precisão.

\cite{b3} O artigo mostra os desafios modernos de NIDS como:
\begin{itemize}
\item \textbf{Volume de Dados}
A crescente quantidade de dados trafegando nas redes torna a detecção de intrusões mais complexa e requer mais velocidade para a continuidade da operação dos sistemas. Isso impõe barreiras entre velocidade e acurácia nas técnicas convencionais de análise.

\item \textbf{Precisão}
As técnicas tradicionais requerem alto grau de intervenção humana, especialmente quando utilizam modelos rasos, que frequentemente se baseiam em outliers, resultando em muitos falsos negativos.

\item \textbf{Diversidade e Dinamismo}
A crescente variedade de dispositivos e protocolos torna desafiador diferenciar tráfego normal de anormal. Além disso, as redes são dinâmicas e mutáveis, dificultando a criação de padrões estáticos para a identificação de ameaças.

\item \textbf{Ataques de Baixa Frequência e Zero-Day}
Esses ataques são particularmente desafiadores para técnicas tradicionais devido ao desbalanceamento ou à inexistência de datasets de treinamento adequados.

\item \textbf{Limitações das Abordagens Tradicionais}
Métodos de \textit{shallow classifiers} de detecção de intrusão possuem muitas limitações, principalmente pela necessidade de grande envolvimento de especialistas humanos para interpretação de dados, identificação de padrões e ajustes de regras. Isso limita a escalabilidade e agilidade na detecção de novas ameaças.

\item \textbf{Deep Learning (DL)}
O \textit{deep learning} tem se destacado como uma solução avançada para modelagem de relações complexas e representações mais abstratas de dados. Dentro desse campo.
\item \textbf{autoencoders} se mostram promissores ao permitirem a extração de características relevantes de forma não-linear, sem necessidade de rótulos. Isso facilita a identificação de anomalias sem um conjunto fixo de assinaturas predefinidas.

\item \textbf{Abordagem Híbrida Proposta}
Para superar as limitações das abordagens clássicas e do deep learning isoladamente, o artigo propõe uma abordagem híbrida utilizando:
\end{itemize}

\begin{itemize}
    \item \textbf{Autoencoders de Deep Learning:} Utilizados para capturar a dinâmica dos padrões de tráfego de rede, reduzindo a necessidade de interação humana e aumentando a capacidade de interpretação genérica.
    \item \textbf{Random Forest:} Aplicado diretamente sobre as saídas do autoencoder, sem a necessidade de um layout achatado (\textit{flatten}), garantindo maior preservação da estrutura dos dados e melhor capacidade de classificação.
\end{itemize}

Essa abordagem híbrida busca aliar a robustez do deep learning à alta acurácia do Random Forest, criando um sistema capaz de lidar com a dinamicidade das redes modernas sem comprometer a confiabilidade na detecção de intrusões.

    \cite{b4} A segurança cibernética enfrenta desafios crescentes devido à explosão no volume e na variedade de ataques. O aumento no número de ataques zero-day torna os sistemas de detecção de intrusão (IDS) baseados em assinaturas menos eficazes, pois dependem de bancos de dados com históricos de ataques conhecidos. Para enfrentar essas limitações, propõe-se uma abordagem baseada na combinação de autoencoders e Support Vector Machine (SVM).

Desafios Atuais na Detecção de Ataques Zero-Day mostram
a dependência de assinaturas históricas limita a detecção de ataques novos e desconhecidos assim como métodos convencionais de detecção de outliers apresentam altas taxas de falso negativo.

As referencias deste estudos indicam que ataques zero-day podem permanecer ativos por um médio de 10 meses antes de serem detectados, comprometendo sistemas durante esse período, e que 62\% dos ataques só são identificados após comprometerem os sistemas. Já 2019, o número de ataques zero-day superou os três anos anteriores, evidenciando a necessidade urgente de modelos mais eficazes.

Os autores do artigo então propõe uma solução de Autoencoder e SVM propondo explora os pontos fortes do deep learning e da SVM para melhorar a detecção de ataques zero-day:
\begin{itemize}
\item Autoencoder: Atua como um detector leve de outliers, capturando padrões de tráfego normais e identificando desvios, o que permite detectar ataques desconhecidos com alta taxa de recall.

\item SVM: Utilizado para classificar os padrões extraídos pelo autoencoder sem a necessidade de um layout achatado (flatten), garantindo maior preservação da estrutura dos dados e melhor capacidade de classificação.
\end{itemize}

Benefícios da Abordagem:
\begin{itemize}

\item Redução da dependência de especialistas humanos para ajustes constantes.

\item Maior capacidade de adaptação a novas ameaças sem necessária reconfiguração manual.

\item Melhor detecção de ataques de baixa frequência e zero-day, que passam despercebidos por abordagens tradicionais.

\end{itemize}

A combinação de autoencoders e SVM representa um passo significativo para sistemas de detecção de intrusão mais eficientes e adaptáveis aos desafios das redes modernas.

    \cite{b5}Otimização do Treinamento de Redes Neurais:
    
    este mostra que estas enfrentam desafios significativos devido à natureza complexa da superfície de custo, que é tipicamente não-quadrática, não-convexa e de alta dimensionalidade. Como resultado, o algoritmo de back propagation pode ser lento e não há garantias de que a convergência para uma solução adequada ocorra de maneira rápida ou sequer aconteça. Para mitigar esses problemas, diversas técnicas foram propostas, incluindo taxas de aprendizado adaptativas e early stopping.
        
    As taxas de aprendizado desempenham um papel crucial na eficiência do treinamento de redes neurais. Diferentes abordagens foram propostas para ajustar automaticamente a taxa de aprendizado com base no erro, de modo a controlar a velocidade de convergência. Uma das estratégias sugeridas é equalizar a velocidade de aprendizado, permitindo que cada peso possua sua própria taxa de aprendizado, proporcional à raiz quadrada do número de entradas da unidade correspondente. Além disso, é recomendado que os pesos das camadas inferiores sejam maiores do que os das camadas superiores para melhor estabilização do treinamento.
    
    O early stopping é uma técnica amplamente utilizada para evitar o overfitting em redes neurais. O processo consiste em interromper o treinamento assim que o erro no conjunto de validação começa a aumentar, impedindo que o modelo se ajuste excessivamente aos dados de treinamento. Os principais passos do early stopping incluem:
\begin{itemize}
    \item \textbf{Divisão dos Dados} O conjunto de treinamento é separado em subconjuntos de treino e validação, frequentemente utilizando uma proporção de 2:1.
    \item \textbf{Monitoramento Periódico} O modelo é treinado apenas com os dados de treino, enquanto a perda no conjunto de validação é verificada periodicamente, por exemplo, a cada cinco épocas.
    \item \textbf{Identificação do Ponto Ideal} Se a perda de validação começar a aumentar em relação à última medição, o treinamento é interrompido.
    \item \textbf{Utilização dos Melhores Pesos} Os pesos da rede no momento imediatamente anterior ao aumento da perda de validação são utilizados como a melhor representação do modelo.
\end{itemize}

    O early stopping assume que a perda de validação é um indicador da capacidade de generalização do modelo, ajudando a melhorar o desempenho em dados não vistos 
    
    Diante dos desafios inerentes ao treinamento de redes neurais, a otimização do processo é essencial para garantir modelos eficazes e eficientes. O uso de taxas de aprendizado adaptativas e early stopping demonstrou ser uma abordagem promissora para mitigar problemas como a convergência lenta e o overfitting. Assim, a seleção criteriosa dessas técnicas pode levar a modelos mais robustos e com melhor capacidade de generalização.

    Porém no mesmo não é considerado como tuning em runtime pu técnicas de limitação de trasholder que são baseadas em interação humanada
        
    %sdn
    \cite{b14} Sistemas de Detecção de Intrusão em Redes (NIDS), Aprendizado de Máquina (ML) e Aprofundado (DL) e Redes Definidas por Software (SDN).
\begin{itemize}
    \item \textbf{NIDS:} Sistemas projetados para detectar atividades maliciosas em redes, como vírus, worms e ataques DDoS. Podem ser baseados em assinatura (detecção de ameaças conhecidas) ou anomalias (detecção de comportamentos incomuns).
     \item \textbf{ML/DL:} Técnicas como SVM, Random Forest, Redes Neurais Convolucionais (CNN) e Autoencoders são aplicadas para melhorar a precisão e reduzir falsos positivos em NIDS.
     \item \textbf{SDN:} Arquitetura que separa os planos de controle e dados, permitindo programabilidade e gerenciamento centralizado. Essa flexibilidade é ideal para implementar NIDS dinâmicos e escaláveis.
    \end{itemize}

    A revisão evidenciou que a combinação de SDN, ML e DL oferece vantagens significativas para NIDS, como maior precisão e adaptabilidade. No entanto, desafios persistem, como a otimização de recursos computacionais e a criação de datasets mais representativos. Como direção futura, propos investigar:
\begin{itemize}
     \item Técnicas híbridas: SOM + Redes Neurais Recorrentes.
     \item Implementação em infraestruturas críticas (ex.: data centers).
\end{itemize}

    %survey current network
   \cite{b15}
Os Sistemas de Detecção de Intrusão em Redes (NIDS) são essenciais para a segurança cibernética, atuando na identificação de atividades maliciosas em tempo real.

O estado da arte em NIDS evoluiu para combinar técnicas reativas (assinaturas) e proativas (anomalias), porém persistem desafios:
\begin{itemize}
    \item \textbf{Limitações:} Falsos positivos, desempenho em redes de alta velocidade e criptografia.
    \item \textbf{Futuro:} Tendência para sistemas distribuídos e autônomos, como NIPS (Prevenção de Intrusão).
\end{itemize}

\noindent \textbf{Abordagem metodológica sugerida:}
\begin{itemize}
    \item \textbf{Quantitativa:} Avaliação de métricas de desempenho (ex.: throughput, taxa de detecção).
    \item \textbf{Qualitativa:} Análise de falsos positivos/negativos em datasets reais (ex.: CICIDS2017).
\end{itemize}

    %performace
    \cite{b16} A análise de sentimentos é uma aplicação crítica de NLP, tradicionalmente baseada em métodos como Bag of Words (BoW) e lexicons, que exigem extração manual de features. Com o avanço do deep learning, técnicas como Word2Vec, GloVe e modelos de ensemble (e.g., NsGA) permitem extração automática de features e melhor desempenho. O artigo propõe um modelo híbrido que combina features superficiais (handcrafted) e profundas (deep learning), superando limitações de generalização e especialização. Segundo Pandian (2021), a integração de vetores de palavras afetivas (NA), genéricas (NG) e superficiais (Ns) resulta em uma melhoria significativa na precisão de classificação (até 95\% F-Score em alguns datasets).

\section{Principais Lacunas Identificadas}
\begin{enumerate}
    \item Dependência de grandes volumes de dados para modelos de deep learning.
    \item Complexidade computacional em ensembles (e.g., NsGA demanda 980s para treinamento).
\end{enumerate}

\section{Considerações Finais}
O estudo demonstra que modelos híbridos (e.g., NsGA) superam abordagens tradicionais em análise de sentimentos, mas exigem otimização para reduzir custos computacionais. Como futuros trabalhos, sugere-se:
\begin{itemize}
    \item Aplicação em outros idiomas (e.g., português).
    \item Uso de técnicas de compressão de modelos (e.g., quantização) para viabilizar implantação em dispositivos edge.
\end{itemize}
